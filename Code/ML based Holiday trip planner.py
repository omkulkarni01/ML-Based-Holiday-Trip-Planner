# -*- coding: utf-8 -*-
"""Capstone Project_Om Kulkarni.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kwahLR6MrGW4MIMKE18nY8I86iPTS4RN

# **Machine Learning based Holiday trip planner**

---

## **Project Description:**

*   Idea behind this project is to develop a Ml based holiday trip planner for creating complete plan of my upcoming trip without any kind of human help.In this project I have used the datasets available on the internet especially on sites like "www.kaggle.com" to predict the best route and price of transportation and also predicted the best hotels at that particular location .
 *   For this project I have used various libraries like sklearn,which contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.Also I have used  plotly Python library which is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases.Along with that I have also used other libraries like pandas,numpy,nltk etc for other basic operations. 
 *   To make it more interactive or user friendly we can colaborate this Ml models with the other website/application platforms like Flask,etc.

## **Objectives:**

Following are some of the main objectives of Ml based holiday trip  planner 
*   To predict the best route and prices of the transportation from available  datasets to reach the desired destination.
*   To predict best hotels with prices based on the previous available data
*   To select suitable Ml models for the above predictions
*   To provide the best and suitable trip plan based on the previous plans selected by the customers/users.

## **Data Source**

1.   [Data_Train.xlsx](https://www.kaggle.com/nikhilmittal/flight-fare-prediction-mh?select=Data_Train.xlsx)
2.  [Test_set.xlsx](https://www.kaggle.com/nikhilmittal/flight-fare-prediction-mh?select=Test_set.xlsx) 
1.[Hotels.csv](https://www.kaggle.com/bimimi/hotel-recommender-system?select=Seattle_Hotels.csv)

## **Import Library**
"""

# ! pip install chart_studio
# ! pip install pyLDAvis
# ! pip install scikit-learn
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score
from math import sqrt
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import StratifiedKFold
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import re
import random
import plotly.graph_objs as go
from chart_studio import plotly   #uncomment above installation lines if library was not found
import cufflinks
pd.options.display.max_columns = 30
from IPython.core.interactiveshell import InteractiveShell
import plotly.figure_factory as ff
InteractiveShell.ast_node_interactivity = 'all'
from plotly.offline import iplot
cufflinks.go_offline()
cufflinks.set_config_file(world_readable=True, theme='solar')

#uncomment Top installation lines if library was not found
import pyLDAvis
import pyLDAvis.sklearn
pyLDAvis.enable_notebook()
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import StratifiedKFold

"""## **Import Data**"""

kfold = StratifiedKFold(n_splits=20)
transport_df = pd.read_excel("/content/drive/MyDrive/Learning Machine /Dataset/Data_Train.xlsx")
test_data = pd.read_excel(r"/content/drive/MyDrive/Learning Machine /Dataset/Test_set.xlsx")
hoteltrain_df = pd.read_csv('/content/drive/MyDrive/Learning Machine /Dataset/Seattle_Hotels.csv', encoding="latin-1")

"""# **Describing Training Dataset**

##Describing transportation training data
"""

transport_df.head(10)
#print(transport_df.head(10)) #uncomment if required

transport_df.columns

transport_df.info()

"""##Describing Hotels data"""

hoteltrain_df.head(10)
#print(hoteltrain_df.head(10)) # uncomment if required

def print_description(index):
  example =hoteltrain_df[hoteltrain_df.index == index][['desc', 'name']].values[0]
  if len(example) > 0:
    print(example[0])
    print('Name:', example[1])
print_description(10)
#print_description(100)

hoteltrain_df['word_count'] = hoteltrain_df['desc'].apply(lambda x: len(str(x).split()))
desc_lengths = list(hoteltrain_df['word_count'])

print("Number of descriptions:",len(desc_lengths),
      "\nAverage word count", np.average(desc_lengths),
      "\nMinimum word count", min(desc_lengths),
      "\nMaximum word count", max(desc_lengths))
hoteltrain_df['word_count'].iplot(
    kind='hist',
    bins = 50,
    linecolor='black',
    xTitle='word count',
    yTitle='count',
    title='Word Count Distribution in Hotel Description')

"""# **Preprocessing of Training data**

##Preprocessing of Transportation data
"""

# Checking if there are any missing values in Training Dataset
transport_df.dropna(inplace = True)
transport_df.isnull().sum()

# After looking at the dataset, there are 4 columns with date-time variables and we need to apply featrure engineering to those columns
#first we take the duration column
transport_df["Duration"].value_counts()

transport_df["Dep_hour"] = pd.to_datetime(transport_df["Dep_Time"]).dt.hour
transport_df.drop(["Dep_Time"], axis = 1, inplace = True)
transport_df.head()

#Now we consider date of journey
transport_df["Journey_day"] = pd.to_datetime(transport_df.Date_of_Journey, format="%d/%m/%Y").dt.day
transport_df["Journey_month"] = pd.to_datetime(transport_df["Date_of_Journey"], format = "%d/%m/%Y").dt.month
transport_df.head()

transport_df.drop(["Date_of_Journey"], axis = 1, inplace = True)

duration = list(transport_df["Duration"]) # Converting 'Duration' column into a list
for i in range(len(duration)):
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins
        if "h" in duration[i]:
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute
        else:
            duration[i] = "0h " + duration[i]           # Adds 0 hour
duration_hours = []
duration_mins = []
for i in range(len(duration)):
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1])) 

transport_df["Duration_hours"] = duration_hours
transport_df["Duration_mins"] = duration_mins
transport_df.drop(["Duration"], axis = 1, inplace = True)

transport_df.head()

transport_df["Airline"].value_counts()

# transport_df[transport_df['Total_Stops'].isnull()]

"""## Preprocessing of Hotels data"""

import nltk
nltk.download('stopwords')
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
        text: a string
        
        return: modified initial string
    """
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text
    
hoteltrain_df['desc_clean'] = hoteltrain_df['desc'].apply(clean_text)

"""# **Data Visualization of Training dataset**

## Visualization of Transportation data
"""

# Airline vs Price
sns.catplot(y = "Price", x = "Airline", data = transport_df.sort_values("Price", ascending = False), kind="bar", height = 6, aspect = 3)
plt.show()

Airline = transport_df[["Airline"]]
Airline = pd.get_dummies(Airline, drop_first= True)
Airline.head()

transport_df["Source"].value_counts()

sns.catplot(y = "Price", x = "Source", data = transport_df.sort_values("Price", ascending = False), kind="bar", height = 4, aspect = 3)
plt.show()

Source = transport_df[["Source"]]

Source = pd.get_dummies(Source, drop_first= True)

Source.head()

transport_df["Destination"].value_counts()

Destination = transport_df[["Destination"]]

Destination = pd.get_dummies(Destination, drop_first = True)

Destination.head()

# Route=transport_df[["Route"]]
# Route=transport_df.drop(["Route", "Additional_Info"], axis = 1, inplace = True) #uncomment to get routes available separetly
transport_df["Total_Stops"].value_counts()

transport_df.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)
transport_df.head()

transport_df = pd.concat([transport_df, Airline, Source, Destination], axis = 1)
transport_df.head()

transport_df.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)
transport_df.head()

transport_df.head()

transport_df.shape

"""##Visualization of Hotel data"""

def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_words(hoteltrain_df['desc'], 20)
hoteltrain_df2 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
hoteltrain_df2.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description after removing stop words')
def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_words(hoteltrain_df['desc'], 20)
hoteltrain_df2 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
hoteltrain_df2.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description after removing stop words')

def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_bigram(hoteltrain_df['desc'], 20)
hoteltrain_df3 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
hoteltrain_df3.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description before removing stop words')


def get_top_n_trigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_trigram(hoteltrain_df['desc'], 20)
hoteltrain_df5 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
hoteltrain_df5.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description before removing stop words')
def get_top_n_trigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_trigram(hoteltrain_df['desc'], 20)
hoteltrain_df6 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
hoteltrain_df6.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description after removing stop words')

"""# **Describing test dataset**"""

test_data = pd.read_excel(r"/content/drive/MyDrive/Learning Machine /Dataset/Test_set.xlsx")
test_data.head()

"""# **Data preprocessing of Test data**"""

print('Training dataset shape:', transport_df.shape)
print('Test dataset shape:', test_data.shape)
test_data.isnull().sum()
#First we consider Duration column
test_data["Duration"].value_counts()
duration_test = list(test_data["Duration"])
duration_test;


for i in range(len(duration_test)):
    if len(duration_test[i].split()) != 2:   
        if "h" in duration_test[i]:
            duration_test[i] = duration_test[i].strip() + ' 0m'  # add 0 minute 
        else:
            duration_test[i] = '0h '+ duration_test[i]           # add 0 hour
            
duration_hours = []
duration_mins = []
for i in range(len(duration_test)):
    duration_hours.append(int(duration_test[i].split(sep = "h")[0]))
    duration_mins.append(int(duration_test[i].split(sep = "m")[0].split()[-1]))
duration_test;
test_data['Duration_hrs'] = duration_hours
test_data['Duration_hrs']

test_data['Duration_mins'] = duration_mins
test_data['Duration_mins']

test_data.drop('Duration', axis=1, inplace=True)
test_data['Day_of_Journey']=pd.to_datetime(test_data['Date_of_Journey'], format='%d/%m/%Y').dt.day
test_data['Month_of_Journey']=pd.to_datetime(test_data['Date_of_Journey'], format='%d/%m/%Y').dt.month
test_data.drop('Date_of_Journey', axis = 1, inplace = True)

test_data['Dep_hr'] = pd.to_datetime(test_data['Dep_Time']).dt.hour
test_data['Dep_min'] = pd.to_datetime(test_data['Dep_Time']).dt.minute
test_data.drop('Dep_Time', axis = 1, inplace = True)

test_data['Arrival_hr'] = pd.to_datetime(test_data['Arrival_Time']).dt.hour
test_data['Arrival_min'] = pd.to_datetime(test_data['Arrival_Time']).dt.minute
test_data.drop('Arrival_Time', axis = 1, inplace = True)
print('Test dataset shape:', test_data.shape)

test= pd.concat([test_data, Airline, Source, Destination], axis = 1)

#Dropping the non-encoded Airline, Source, Destination variables
test.drop(['Airline', 'Source', 'Destination', 'Additional_Info', 'Route'], axis = 1, inplace = True)
#dropping route column as we have a stop column which basically covers the entire zest of it 


#Let's take care of Total_Stops
test.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
test['Total_Stops'] = encoder.fit_transform(test['Total_Stops'])

print(test.shape)
test.head()

transport_df.shape

test.columns

transport_df.columns

# price=transport_df.Price
# transport_df.drop('Price', axis=1, inplace=True)
# data_df=data_df.join(price)
# data_df.head()

"""## **Define Target Variable (y) and Feature Variables (X)**"""

X = transport_df.loc[:,['Total_Stops','Price','Dep_hour', 'Duration_hours',
       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',
       'Airline_Jet Airways', 'Airline_Jet Airways Business',
       'Airline_Multiple carriers',
       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',
       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',
       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',
       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',
       'Destination_Kolkata', 'Destination_New Delhi']]

X.head()

y = transport_df.iloc[:,4]
y

# # Finds correlation between Independent and dependent attributes

# plt.figure(figsize = (18,18))
# sns.heatmap(transport_df.corr(), annot = True, cmap = "RdYlGn")

# plt.show()

# Important feature using ExtraTreesRegressor

from sklearn.ensemble import ExtraTreesRegressor
selection = ExtraTreesRegressor()
selection.fit(X, y)
print(selection.feature_importances_)

#bar graph of feature importances 
plt.figure(figsize = (10,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

"""##**Train Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

"""#**Training Models for predictions**

##**Transportation**

We are using four types of Ml methods out of which we are selecting the best accuracy giving method based on specific datasets

###**Using Linear Regression**
"""

from sklearn.linear_model import LinearRegression
lin_reg=LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred=lin_reg.predict(X_test)

print("Linear Regression Score on Training set is",lin_reg.score(X_train, y_train))#Training Accuracy
print("Linear Regression Score on Test Set is",lin_reg.score(X_test, y_test))#Testing Accuracy

accuracies = cross_val_score(lin_reg, X_train, y_train, cv = kfold)
print(accuracies)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

mae=mean_absolute_error(y_pred, y_test)
print("Mean Absolute Error:" , mae)

mse=mse(y_test, y_pred)
print("Mean Squared Error:" , mse)

print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

print('The r2_score is', metrics.r2_score(y_test, y_pred))

"""###**Decision Tree Regression**"""

#Decision Tree Regressor
# from sklearn.tree import DecisionTreeRegressor
dt_reg = DecisionTreeRegressor(random_state = 0)
dt_reg.fit(X_train, y_train)
y_pred=dt_reg.predict(X_test)

print("Decision Tree Score on Training set is",dt_reg.score(X_train, y_train))#Training Accuracy
print("Decision Tree Score on Test Set is",dt_reg.score(X_test, y_test))#Testing Accuracy

accuracies = cross_val_score(dt_reg, X_train, y_train, cv = kfold)
print(accuracies)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

mae=mean_absolute_error(y_pred, y_test)
print("Mean Absolute Error:" , mae)

mse=mse(y_test, y_pred)
print("Mean Squared Error:" , mse)

print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

print('The r2_score is', metrics.r2_score(y_test, y_pred))

"""###**Fitting model using Random Forest**"""

# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestRegressor
rf_reg = RandomForestRegressor(n_estimators=400,min_samples_split=15,min_samples_leaf=2,
max_features='auto', max_depth=30)
rf_reg.fit(X_train, y_train)
y_pred=rf_reg.predict(X_test)
print("Random Forest Score on Training set is",rf_reg.score(X_train, y_train))#Training Accuracy
print("Random Forest Score on Test Set is",rf_reg.score(X_test, y_test))#Testing Accuracy
rf_reg.score(X_test, y_test)
accuracies = cross_val_score(rf_reg, X_train, y_train, cv = kfold)
print(accuracies)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

mae=mean_absolute_error(y_pred, y_test)
print("Mean Absolute Error:" , mae)

mse=mse(y_test, y_pred)
print("Mean Squared Error:" , mse)

print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

print('The r2_score is', metrics.r2_score(y_test, y_pred))

sns.distplot(y_test-y_pred)
plt.show()

plt.scatter(y_test, y_pred, alpha = 0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""###**Randomized Search CV**"""

#Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
# Create the random grid

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}
# Random search of parameters, using 5 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = rf_reg, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
rf_random.fit(X_train,y_train)

rf_random.best_params_
prediction = rf_random.predict(X_test)
plt.figure(figsize = (8,8))
sns.distplot(y_test-prediction)
plt.show()

plt.figure(figsize = (8,8))
plt.scatter(y_test, prediction, alpha = 0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

print('MAE:', metrics.mean_absolute_error(y_test, prediction))
print('MSE:', metrics.mean_squared_error(y_test, prediction))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))

"""##**Hotels**

At first we are creating TfidfVectorizer object and then we generate matrix of word vectors for using it into predictive modelling and at the last we compute the cosine similarity matrix for performing various predictions/recommendations.
"""

# hoteltrain_df.set_index('name', inplace = True)
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(hoteltrain_df['desc_clean'])
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
indices = pd.Series(hoteltrain_df.index)
indices[:50]

"""#**Saving & Predicting the Prices for the Holiday Trips**

At this stage we predict and recommend the best tarvel plan and hotels to the customer

##**Predicting the prices of Transportation(Airway)**
"""

import pickle
# open a file, where you ant to store the data
file = open('flight_rf.pkl', 'wb')

# dump information to that file
pickle.dump(rf_reg, file)
model = open('rf_reg.pkl','rb')
forest = pickle.load(model)
y_prediction = forest.predict(X_test)
metrics.r2_score(y_test, y_prediction)

"""##**Hotels Predictions/recommendations** """

def recommendations(name, cosine_similarities = cosine_similarities):
    
    recommended_hotels = []
    
    # gettin the index of the hotel that matches the name
    idx = indices[indices == name].index[0]

    # creating a Series with the similarity scores in descending order
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)

    # getting the indexes of the 10 most similar hotels except itself
    top_10_indexes = list(score_series.iloc[1:11].index)
    
    # populating the list with the names of the top 10 matching hotels
    for i in top_10_indexes:
        recommended_hotels.append(list(hoteltrain_df.index)[i])
        
    return recommended_hotels

recommendations('Hilton Seattle Airport & Conference Center')